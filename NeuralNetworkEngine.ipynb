{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network Engine From Scratch",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpm0a-_ClKjM"
      },
      "source": [
        "*A quick note: You can run all of the code blocks below by hovering your mouse over the brackets at the top left of each block and clicking the play button that appears. In order to run a block of code, you have to have already run the subsequent blocks (In other words you have to run the code blocks in order)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMNfXg_IlTU4"
      },
      "source": [
        "## **Creating A Neural Network Engine From Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ0d2X6Dpa9c"
      },
      "source": [
        "**What is a Neural Network?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OnR1sFZpoPK"
      },
      "source": [
        "The first thing you might be wondering is what a neural network is. I won't go into too much detail here but in essence, a **neural network** is an algorithm that is intented to simulate some of the features of the human brain that is used to interpret complex data and draw conclusions by making connections between different data points. If you want to learn more about the specifics of neural networks, you can chek out [this video](https://www.youtube.com/results?search_query=3blue1brown+neural+network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR-aOhrtlu2L"
      },
      "source": [
        "**About This Python Notebook**\n",
        "\n",
        "In this Python notebook, I will explain how I created my own object oriented neural network engine with no external dependencies (the only libraries I used were default Python libraries so no data science tools like numpy or pandas and no neural network libraries like tensorflow/keras, PyTorch, scikit, etc.). This tool can be used in a similar way to the traditionally used Python libraries for creating neural networks like tensorflow/keras. \n",
        "\n",
        "In order to gain a thorough fundamental understanding of exactly how neural networks work (especially the intuition and calculus behind backpropagation), I decided to creat this neural network framework from scratch to understand what is happening at every step of a neural network's process.\n",
        "\n",
        "This will be more of a technical Python notebook because I can't split up the code into multiple code blocks (since I am using object oriented programming and the majority of the code is inside the NeuralNetwork object, I can't split up the code because all of the methods for the object need to be in one block). \n",
        "\n",
        "I will be posting a higher-level/fundamental explanation of everything going on here on [My Medium](https://medium.com/@mr.adam.maj) soon so make sure to check there if you are looking for a simpler explanation.\n",
        "\n",
        "Since I cant split up the code, I will explain all of the code through in-code comments rather than text boxes. Look out for red text inside of triple quotes (\"\"\") or green text following hashtags (#) as these are the comments that I have left explaining the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oUlHDfko1m6"
      },
      "source": [
        "# **Creating a Neural Network Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg2tPkR_o8cT"
      },
      "source": [
        "In the code below, we create a NeuralNetwork class using object orient programming. Instances of this class will be individual neural networks, with customizable layers. The idea is that the entire neural network will be fully customizable from the number of layers in the network to the number of neurons in each layer to the activation functions of each neuron.\n",
        "\n",
        "**Run the code block below if you want to follow along when we create and train a neural network later.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfZJwSKjjj1C"
      },
      "source": [
        "from random import random\n",
        "import math\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    \"\"\"\n",
        "    This class will create an instance of a neural network. The idea is that we \n",
        "    will be able to use the methods in this class to customize the neural network\n",
        "    to the exact size that we want it to be (including the number of layers, number\n",
        "    of neurons per layer, and the activations of the neurons). This means that we\n",
        "    will need our forward and backpropgation algorithms to be programmed so that\n",
        "    they will function properly no matter the size of the network.\n",
        "\n",
        "    The code for this class is split into four main steps:\n",
        "    1. Initialization: In this section, we enable the creation and customization \n",
        "       of the neural network (adding layers, changing activations, etc.)\n",
        "    2. Forward Propagation: In this section, we create the forward propagation\n",
        "       algorithm which will both store the outputs of each neuron during forward\n",
        "       propagation and will return the output of the network (thus it can be used)\n",
        "       when our network is fully trained to make predictions.\n",
        "    3. Backward Propagation: In this section, we create the backward propagation\n",
        "       algorithm which calculates the errors of each neuron in the network based\n",
        "       on the error of the network calculated after forward propagation and adjusts\n",
        "       the weights of each neuron accordingly.\n",
        "    4. Training: In this section, we create the function which will enable us to\n",
        "       train our network with specific training data. This is where we will be able \n",
        "       to specify relevant learning parameters like the learning rate and number of\n",
        "       epochs that we want.\n",
        "    \"\"\"\n",
        "\n",
        "    # Section 1: Initialization \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        When we first initialize the network, we don't need to do much. All we do\n",
        "        here is create an empty list for our neural network (which can be filled)\n",
        "        in later.\n",
        "        \"\"\"\n",
        "        self.neural_network = list()\n",
        "    \n",
        "    def add_layer(self, num_neurons, activation = 'sigmoid', input_layer = False):\n",
        "        \"\"\"\n",
        "        This is the function that allows the user to add layers to the neural \n",
        "        network. They can specify the number of neurons in the layer, the activation\n",
        "        function used by the neurons in the layer (which is set by default to the\n",
        "        sigmoid function but can be adjusted), and if the layer is the input layer.\n",
        "        Note that the first layer added to the network must always be specified \n",
        "        as the input layer.\n",
        "        \"\"\"\n",
        "        if not input_layer:\n",
        "            #Each layer is represented as a list of neurons where each neuron\n",
        "            #is represented as a dictionary with a bias, a set of weights corresponding\n",
        "            #to the neurons in the previous layer, and an activation function.\n",
        "            self.neural_network.append([{'bias': random(), \n",
        "                                         'weights': [random() for i in range(self.last_layer_neurons)],\n",
        "                                         'activation': activation} \n",
        "                                         for i in range(num_neurons)])\n",
        "        self.last_layer_neurons = num_neurons\n",
        "\n",
        "    # Section 2: Forward Propagation\n",
        "    def sigmoid(self, value, derivative = False):\n",
        "        \"\"\"\n",
        "        This is the Sigmoid activation function which can be used in computing the\n",
        "        activation of a neuron. It can be also used as the derivative of the sigmoid\n",
        "        function (which you will see we need later in backpropagation).\n",
        "        \"\"\"\n",
        "        if derivative:\n",
        "            return value * (1 - value)\n",
        "        return 1/(1 + math.exp(-1 * value))\n",
        "    \n",
        "    def relu(self, value, derivative = False):\n",
        "        \"\"\"\n",
        "        This is the Rectified Linear Unit (ReLU) activation function which can be \n",
        "        used in computing the activation of a neuron. This function returns a value \n",
        "        of 0 if it is fed a value less than 0 and returns the value it was fed if \n",
        "        the value is greater than 0. It can also be used as the derivative of the \n",
        "        relu function (which again we will need in backpropagation).\n",
        "        \"\"\"\n",
        "        if derivative:\n",
        "            return 1 if value > 0 else 0\n",
        "        return value if value > 0 else 0\n",
        "    \n",
        "    def activate(self, inputs, weights, bias, activation):\n",
        "        \"\"\"\n",
        "        This function computes the activation of a specific neuron. It computes\n",
        "        the dot product of the input and weight vectors (basically just multiplying\n",
        "        all of the weights coming into the neuron with the corresponding outputs\n",
        "        of the neurons they are coming from and then adding all of them up) and \n",
        "        then adds the bias to this number before passing the whole sum into the\n",
        "        specified activation function.\n",
        "        \"\"\"\n",
        "        activation_functions = {'sigmoid': self.sigmoid, 'relu': self.relu}\n",
        "        activation_function = activation_functions[activation]\n",
        "        \n",
        "        return activation_function(sum([inputs[i] * weights[i] for i in range(len(inputs))]) + bias)\n",
        "    \n",
        "    def forward_propagate(self, inputs):\n",
        "        \"\"\"\n",
        "        This function passes the specified inputs into the input layer of the neural\n",
        "        network. From there, it computes the activation of each neuron in a layer \n",
        "        using the fuction above and then passes these activations to the neurons\n",
        "        of the next layer, repeating this process until it reaches the output layer.\n",
        "        \"\"\"\n",
        "        for layer in self.neural_network:\n",
        "            layer_outputs = list()\n",
        "            for neuron in layer:\n",
        "                #Here we store the outputs of each neuron (computed using the activate\n",
        "                #function we defined above) because we will need them for backpropagation\n",
        "                neuron['output'] = self.activate(inputs, neuron['weights'], neuron['bias'], neuron['activation'])\n",
        "                layer_outputs.append(neuron['output'])\n",
        "            #Here is where we pass the outputs of one layer as the inputs of the next layer\n",
        "            inputs = layer_outputs\n",
        "        #Here we return the output of the final layer (which is the prediction of the network)\n",
        "        return inputs\n",
        "    \n",
        "    # Section 3: Backward Propagation\n",
        "    def backward_propagate(self, inputs, expected_outputs, learning_rate):\n",
        "        \"\"\"\n",
        "        This function calculates the error of our network (the difference between\n",
        "        the output predicted by the network and the output it was supposed to predict).\n",
        "        In then uses this error of the last neurons in the network to calculate the \n",
        "        error of the neurons in the previous layer using some simple calculus. \n",
        "        Then, it adjust the weights and biases of our neural network based on the\n",
        "        errors of each neuron (this is what actually makes the network more accurate).\n",
        "        \"\"\"\n",
        "        #Here, we compute the errors of each neuron, focusing on the layers\n",
        "        #In reverse order where we start by looking at the last layer\n",
        "        for i in reversed(range(len(self.neural_network))):\n",
        "            layer = self.neural_network[i]\n",
        "            layer_errors = list()\n",
        "            #If we are looking at the last layer...\n",
        "            if i == len(self.neural_network) - 1:\n",
        "                for j in range(len(layer)):\n",
        "                    neuron = layer[j]\n",
        "                    #Compute the error of the output neurons in our neural network\n",
        "                    neuron_error = expected_outputs[j] - neuron['output']\n",
        "                    layer_errors.append(neuron_error)\n",
        "            else:\n",
        "                #For any other layer besides the last layer...\n",
        "                for j in range(len(layer)):\n",
        "                    #Calculate the error associated with each neuron based on the \n",
        "                    #connected neurons in the following layer\n",
        "                    neuron_error = sum([neuron['weights'][j] * neuron['error'] for neuron in self.neural_network[i + 1]])\n",
        "                    layer_errors.append(neuron_error)\n",
        "            for j in range(len(layer)):\n",
        "                neuron = layer[j]\n",
        "                #Finally, multiply the errors we found before by the derivative of \n",
        "                #activation function of each neuron (this comes from using the chain\n",
        "                #rule from calculus).\n",
        "                neuron['error'] = layer_errors[j] * self.sigmoid(neuron['output'], True)\n",
        "        \n",
        "        #Here, we adjust the weights of our network based on the errors we computed\n",
        "        #This time we go through the neural network in order from the first to last layer\n",
        "        for i in range(len(self.neural_network)):\n",
        "            layer = self.neural_network[i]\n",
        "            for neuron in layer:\n",
        "                for j in range(len(neuron['weights'])):\n",
        "                    #Here we adjust the weights based on the error associated with\n",
        "                    #each neuron and the outputs of the previous layer. We also\n",
        "                    #take into account the learning rate to specify how much we want\n",
        "                    #to adjust our weights for each training example.\n",
        "                    neuron['weights'][j] += learning_rate * inputs[j] * neuron['error']\n",
        "                #Here we adjust the bias taking into account the learning rate\n",
        "                neuron['bias'] += learning_rate * neuron['error']\n",
        "            #Here we save the outputs of the current layer so that we can reference them\n",
        "            #When adjusting the weights of the next layer.\n",
        "            inputs = [neuron['output'] for neuron in self.neural_network[i]]\n",
        "    \n",
        "    #Section 4: Training\n",
        "    def train(self, x_train, y_train, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        Finally, we create the method which we wil use to access all of the above\n",
        "        methods. This method will allow us to pass in training data into our neural\n",
        "        network. The network will train with the specified number of epochs (iterations\n",
        "        through the training data) and will use the specified learning rate.\n",
        "        \n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            total_error = 0\n",
        "            for i in range(len(x_train)):\n",
        "                x = x_train[i]\n",
        "                y = y_train[i]\n",
        "                #Propagate our training inputs through our network\n",
        "                outputs = self.forward_propagate(x)\n",
        "                #Calculate the mean squared error of the predicted outputs (for display purposes)\n",
        "                total_error += sum([(y[j] - outputs[j]) ** 2 for j in range(len(outputs))])\n",
        "                #Adjust weights and biases with backpropagation\n",
        "                self.backward_propagate(x, y, learning_rate)\n",
        "            #Print a message to the console at the end of each epoch so we can see\n",
        "            #The progress of the neural network/if it is improving\n",
        "            print(\"Epoch: {}, Total Error: {:.3f}\".format(epoch, total_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5IhbNhlq0Xr"
      },
      "source": [
        "# **Building & Training A Simple Neural Network With The Engine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFEEnqprgGO"
      },
      "source": [
        "In the code below, we create a neural network using our NeuralNetwork class that we created above. For the sake of simplicity, I will use a very basic sample dataset with a few datapoints (that way we can train our network quickly and see the results). However, note that this neural network engine is functional for much larger datasets as you can create as many layers and nodes as you want. If you want to see this neural network engine used in a more complex and applicable use case, check out [this Python notebook](https://colab.research.google.com/drive/1FWyLbDmi415bUFFmRhVFbLogtswRoHkd?usp=sharing) where I create an artificial neural network to analyze bank information using my neural network engine.\n",
        "\n",
        "The dataset I will be using is from [this resource](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/), which also goes over a higher level explanation of the code in this notebook (I adapted this code to make it object oriented, more readable, and simplified but also more powerful).\n",
        "\n",
        "**Run the Code Below to Load In the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTN_P5KcsGkX"
      },
      "source": [
        " #Each training example has two data points\n",
        " x_train = [[2.7810836, 2.550537003],\n",
        "\t          [1.465489372, 2.362125076],\n",
        "\t          [3.396561688, 4.400293529],\n",
        "\t          [1.38807019, 1.850220317],\n",
        "\t          [3.06407232, 3.005305973],\n",
        "\t          [7.627531214, 2.759262235],\n",
        "\t          [5.332441248, 2.088626775],\n",
        "\t          [6.922596716, 1.77106367],\n",
        "\t          [8.675418651, -0.242068655],\n",
        "\t          [7.673756466, 3.508563011]]\n",
        "\t\t\t\t\t\t\n",
        "#Each of these outputs is meant to represent a binary value where [1, 0] represents\n",
        "#0 and [0, 1] represents 1.\n",
        "y_train = [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKEbjmGxwyYq"
      },
      "source": [
        "Now that we have the simple dataset we will be using, let's create our neural network with the engine that we created. Since each training input has two values, we will create an input layer for our network with two neurons. Next, we will add a hidden layer with two neurons, and finally an output layer with two neurons, one to predict an output of 0 and one to predict an output of 1.\n",
        "\n",
        "**Run the Code Below to Create Our Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0PwPZWAxSxc"
      },
      "source": [
        "#Create an instance of our NeuralNetwork class\n",
        "model = NeuralNetwork()\n",
        "\n",
        "#Add the input layer, hidden layer, and output layer, each with two neurons\n",
        "model.add_layer(2, input_layer = True)\n",
        "model.add_layer(2)\n",
        "model.add_layer(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnCuS1fMxVPd"
      },
      "source": [
        "Now that we have our neural network, we can train the network and see it in action. Run the code below to train our model with the dataset we loaded in earlier. Notice how the error is decreasing as the neural networks is training and gets down to a very low number. That means that the neural network is working as intended!\n",
        "\n",
        "**Run the Code Below to Train Our Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Gmq0EUyg9J"
      },
      "source": [
        "#Train our network with the datasets specified above. We are using a learning rate of 0.5\n",
        "#And are training with 100 epochs.\n",
        "model.train(x_train, y_train, 0.5, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZYKoUBWu94M"
      },
      "source": [
        "# **Thanks for Reading!**\n",
        "\n",
        "My name is Adam Majmudar and I'm a 17 year old machine learning developer.\n",
        "\n",
        "If you want to connect with me/see my work, check out the following links:\n",
        "\n",
        "* [My Medium](https://medium.com/@mr.adam.maj)\n",
        "* [My Linkedin](https://www.linkedin.com/in/adam-majmudar-24b596194/)\n",
        "* [My Portfolio](https://tks.life/profile/adam.majmudar)\n",
        "\n",
        "\n",
        "If you enjoyed this article, make sure to check out the following:\n",
        "\n",
        "* [Creating a Neural Network to Analyze Data About Bank Customers With My Neural Network Engine](https://colab.research.google.com/drive/1FWyLbDmi415bUFFmRhVFbLogtswRoHkd?usp=sharing): In this notebook, I create a more advanced neural network with my neural network engine to showcase it being used to solve a real problem. \n",
        "\n",
        "* [How to Code a Neural Network With Backpropagation in Python](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/): This notebook gives an excellent explanation of the concepts used in this code. I adapted the code in this article into an object oriented format so that it could be applied to a number of different problems. Additonally, I recreated the same concepts with my own code and different logic in specific areas to make the code more understandable and functional."
      ]
    }
  ]
}